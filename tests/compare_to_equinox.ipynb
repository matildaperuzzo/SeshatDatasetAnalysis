{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a1a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..')))\n",
    "# Now you can import the TimeSeriesDataset class\n",
    "from src.TimeSeriesDataset import TimeSeriesDataset as TSD\n",
    "from src.utils import download_data, standardize_column, download_data_json, longest_substring_finder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b6793b",
   "metadata": {},
   "source": [
    "Save Polaris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bd1b33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "translation = {\n",
    "    \"absent\":\"A\",\n",
    "    \"present\":\"P\",\n",
    "    \"unknown\":\"U\",\n",
    "\n",
    "}\n",
    "def get_code(coded_variable, tag):\n",
    "    if pd.isna(coded_variable):\n",
    "        coded_variable = \"\"\n",
    "    elif coded_variable.lower() in translation:\n",
    "        coded_variable = translation[coded_variable.lower()]\n",
    "    elif coded_variable == \"uncoded\":\n",
    "        coded_variable = np.nan\n",
    "        return coded_variable\n",
    "    else:\n",
    "        return coded_variable\n",
    "    if tag == 'TRS':\n",
    "        return coded_variable\n",
    "    elif tag == 'IFR':\n",
    "        return \"I\" + coded_variable\n",
    "    elif tag == \"SSP\":\n",
    "        return \"S\" + coded_variable\n",
    "    elif tag == \"UND\":\n",
    "        return \"UND \" + coded_variable\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown tag: {tag}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a04b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pandas import json_normalize\n",
    "response = requests.get('https://seshat-db.com/api/', timeout=5)\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "    # Normalize the JSON data into a DataFrame\n",
    "    urls = json_normalize(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d58c397f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 862 rows\n"
     ]
    }
   ],
   "source": [
    "polity_df = download_data('https://seshat-db.com/api/core/polities/')\n",
    "polity_df = polity_df[['id','name','long_name','home_seshat_region_name','home_seshat_region_subregions_list',  'start_year', 'end_year']]\n",
    "polity_df = polity_df.rename(columns={'id':'polity_number','name':'polity_id','home_seshat_region_name': 'seshat_region','home_seshat_region_subregions_list': 'seshat_subregions_list'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7555198c",
   "metadata": {},
   "outputs": [],
   "source": [
    "threads = pd.read_excel('..\\datasets\\cliopatria_nga_locations.xlsx')\n",
    "polity_df['thread_num'] = polity_df.polity_id.apply(lambda x: threads.loc[threads['polity_id'] == x, 'thread_id'].values[0] if x in threads['polity_id'].values else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c172aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_variables = []\n",
    "api_variables = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "454a94a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = {\"alternative_names\": \"alternative_name\",\n",
    "                \"supra-polity_relations\": \"supra_polity_relations\",\n",
    "                \"polity_capital\" : \"capital\",\n",
    "                \"relationship_to_preceding_(quasi)polity\" : \"relationship_to_preceding_entity\",\n",
    "                \"preceding_(quasi)polity\" : \"preceding_entity\",\n",
    "                \"succeeding_(quasi)polity\" : \"succeeding_entity\",\n",
    "                \"scale_of_supra-cultural_interaction\" : \"scale\",\n",
    "                # \"polity_original_name\" : \"original_name\",\n",
    "                # 'polity_alternative_name' : 'alternative_name',\n",
    "                # 'polity_degree_of_centralization' : 'degree_of_centralization',\n",
    "                'polity_suprapolity_relations' : 'supra_polity_relations',\n",
    "                # 'polity_utm_zone' : 'utm_zone',\n",
    "                # 'polity_language' : 'language',\n",
    "                # 'polity_linguistic_family' : 'linguistic_family',\n",
    "                # 'polity_language_genus' : 'language_genus',\n",
    "                # 'polity_religion_genus' : 'religion_genus',\n",
    "                # 'polity_religion_family' : 'religion_family',   \n",
    "                # 'polity_religion' : 'religion',\n",
    "                'polity_scale_of_supracultural_interaction' : 'scale',\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "356924cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "general_urls = []\n",
    "for col in urls.columns:\n",
    "    if col.split(\"/\")[-2] == \"general\":\n",
    "        general_urls.append(urls[col].values[0])\n",
    "general_urls = general_urls[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "420a1481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 501 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 1331 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 499 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 382 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 500 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 629 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 630 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 521 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 43 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 139 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 149 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 44 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 367 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 452 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 457 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 272 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 132 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 43 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 33 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Downloaded 106 rows\n",
      "All values in 'name' are the same. No changes needed.\n",
      "Empty dataframe for URL: https://seshat-db.com/api/general/polity-experts/\n",
      "Empty dataframe for URL: https://seshat-db.com/api/general/polity-editors/\n",
      "Downloaded 168 rows\n",
      "All values in 'name' are the same. No changes needed.\n"
     ]
    }
   ],
   "source": [
    "general_db = pd.DataFrame()\n",
    "for url in general_urls:\n",
    "    if url.split('/')[-2] in [\"polity-durations\", \"polity-peak-years\"]:\n",
    "        continue\n",
    "    var_df = download_data(url)\n",
    "    if len(var_df) == 0:\n",
    "        print(f\"Empty dataframe for URL: {url}\")\n",
    "        continue\n",
    "    url_name = url.split('/')[-2]\n",
    "    var_df = standardize_column(var_df, \"name\")\n",
    "    variable_name = var_df.name.iloc[0].lower()\n",
    "    sql_variables.append(variable_name)\n",
    "    api_variables.append(url_name)\n",
    "    row_variable_name = variable_name\n",
    "    if (variable_name not in var_df.columns) and (variable_name + \"_from\" not in var_df.columns):\n",
    "        if (variable_name in column_names):\n",
    "            row_variable_name = column_names[variable_name].lower()\n",
    "        elif (variable_name.split(\"_\")[0] == 'polity') and ('_'.join(variable_name.split('_')[1:]) in var_df.columns):\n",
    "            row_variable_name = '_'.join(variable_name.split('_')[1:])\n",
    "        else:\n",
    "            row_variable_name = \"coded_value\"\n",
    "   \n",
    "    range_var =  row_variable_name + \"_from\" in var_df.columns\n",
    "    if range_var:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df[row_variable_name + '_from'],\n",
    "            \"value_to\": var_df[row_variable_name + '_to'],\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    else:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df.apply(lambda x: get_code(x[row_variable_name], x['tag']), axis=1),\n",
    "            \"value_to\": np.nan,\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    general_db = pd.concat([general_db, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66951644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty dataframe for URL: https://seshat-db.com/api/sc/research-assistants/\n",
      "Downloaded 606 rows\n",
      "Downloaded 579 rows\n",
      "Downloaded 582 rows\n",
      "Downloaded 563 rows\n",
      "Downloaded 570 rows\n",
      "Downloaded 446 rows\n",
      "Downloaded 448 rows\n",
      "Downloaded 473 rows\n",
      "Downloaded 500 rows\n",
      "Downloaded 464 rows\n",
      "Downloaded 494 rows\n",
      "Downloaded 385 rows\n",
      "Downloaded 398 rows\n",
      "Downloaded 479 rows\n",
      "Downloaded 509 rows\n",
      "Downloaded 478 rows\n",
      "Downloaded 435 rows\n",
      "Downloaded 414 rows\n",
      "Downloaded 468 rows\n",
      "Downloaded 398 rows\n",
      "Downloaded 501 rows\n",
      "Downloaded 447 rows\n",
      "Downloaded 466 rows\n",
      "Downloaded 373 rows\n",
      "Downloaded 368 rows\n",
      "Downloaded 448 rows\n",
      "Downloaded 366 rows\n",
      "Downloaded 243 rows\n",
      "Downloaded 339 rows\n",
      "Downloaded 584 rows\n",
      "Downloaded 578 rows\n",
      "Downloaded 349 rows\n",
      "Downloaded 496 rows\n",
      "Downloaded 483 rows\n",
      "Downloaded 531 rows\n",
      "Downloaded 546 rows\n",
      "Downloaded 561 rows\n",
      "Downloaded 500 rows\n",
      "Downloaded 489 rows\n",
      "Downloaded 461 rows\n",
      "Downloaded 482 rows\n",
      "Downloaded 499 rows\n",
      "Downloaded 465 rows\n",
      "Downloaded 394 rows\n",
      "Downloaded 398 rows\n",
      "Downloaded 448 rows\n",
      "Downloaded 521 rows\n",
      "Downloaded 459 rows\n",
      "Downloaded 410 rows\n",
      "Downloaded 389 rows\n",
      "Downloaded 387 rows\n",
      "Downloaded 132 rows\n",
      "Downloaded 161 rows\n",
      "Downloaded 126 rows\n",
      "Downloaded 86 rows\n",
      "Downloaded 127 rows\n",
      "Downloaded 10 rows\n",
      "Downloaded 105 rows\n",
      "Downloaded 105 rows\n",
      "Downloaded 102 rows\n",
      "Downloaded 95 rows\n",
      "Downloaded 67 rows\n",
      "Downloaded 125 rows\n",
      "Downloaded 112 rows\n",
      "Downloaded 125 rows\n",
      "Downloaded 140 rows\n",
      "Downloaded 111 rows\n",
      "Downloaded 88 rows\n",
      "Downloaded 25 rows\n",
      "Downloaded 96 rows\n",
      "Downloaded 73 rows\n",
      "Downloaded 108 rows\n",
      "Downloaded 75 rows\n",
      "Downloaded 69 rows\n",
      "Downloaded 29 rows\n",
      "Downloaded 73 rows\n",
      "Downloaded 49 rows\n"
     ]
    }
   ],
   "source": [
    "sc_urls = []\n",
    "for col in urls.columns:\n",
    "    if col.split(\"/\")[-2] == \"sc\":\n",
    "        sc_urls.append(urls[col].values[0])\n",
    "        \n",
    "sc_db =pd.DataFrame()\n",
    "for url in sc_urls:\n",
    "    var_df = download_data(url)\n",
    "    if len(var_df) == 0:\n",
    "        print(f\"Empty dataframe for URL: {url}\")\n",
    "        continue\n",
    "    \n",
    "    url_name = url.split('/')[-2]\n",
    "    variable_name = var_df.name.iloc[0].lower()\n",
    "    sql_variables.append(variable_name)\n",
    "    api_variables.append(url_name)\n",
    "    row_variable_name = variable_name\n",
    "    if (variable_name not in var_df.columns) and (variable_name + \"_from\" not in var_df.columns):\n",
    "        row_variable_name = \"coded_value\"\n",
    "    range_var =  row_variable_name + \"_from\" in var_df.columns\n",
    "    if range_var:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df[row_variable_name + '_from'],\n",
    "            \"value_to\": var_df[row_variable_name + '_to'],\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    else:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df.apply(lambda x: get_code(x[row_variable_name], x['tag']), axis=1),\n",
    "            \"value_to\": np.nan,\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    new_df = new_df[new_df['polity_id'].notna()]\n",
    "    sc_db = pd.concat([sc_db, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2dc712b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 273 rows\n",
      "Downloaded 338 rows\n",
      "Downloaded 363 rows\n",
      "Downloaded 378 rows\n",
      "Downloaded 375 rows\n",
      "Downloaded 373 rows\n",
      "Downloaded 365 rows\n",
      "Downloaded 368 rows\n",
      "Downloaded 374 rows\n",
      "Downloaded 369 rows\n",
      "Downloaded 349 rows\n",
      "Downloaded 344 rows\n",
      "Downloaded 353 rows\n",
      "Downloaded 362 rows\n",
      "Downloaded 364 rows\n",
      "Downloaded 362 rows\n",
      "Downloaded 364 rows\n",
      "Downloaded 372 rows\n",
      "Downloaded 375 rows\n",
      "Downloaded 375 rows\n",
      "Downloaded 349 rows\n",
      "Downloaded 333 rows\n",
      "Downloaded 354 rows\n",
      "Downloaded 375 rows\n",
      "Downloaded 362 rows\n",
      "Downloaded 344 rows\n",
      "Downloaded 337 rows\n",
      "Downloaded 354 rows\n",
      "Downloaded 374 rows\n",
      "Downloaded 375 rows\n",
      "Downloaded 362 rows\n",
      "Downloaded 367 rows\n",
      "Downloaded 351 rows\n",
      "Downloaded 347 rows\n",
      "Downloaded 358 rows\n",
      "Downloaded 333 rows\n",
      "Downloaded 314 rows\n",
      "Downloaded 333 rows\n",
      "Downloaded 361 rows\n",
      "Downloaded 369 rows\n",
      "Downloaded 368 rows\n",
      "Downloaded 366 rows\n",
      "Downloaded 370 rows\n",
      "Downloaded 363 rows\n",
      "Downloaded 372 rows\n",
      "Downloaded 360 rows\n",
      "Downloaded 367 rows\n",
      "Downloaded 368 rows\n",
      "Downloaded 354 rows\n"
     ]
    }
   ],
   "source": [
    "wf_urls = []\n",
    "for col in urls.columns:\n",
    "    if col.split(\"/\")[-2] == \"wf\":\n",
    "        wf_urls.append(urls[col].values[0])\n",
    "        \n",
    "wf_db =pd.DataFrame()\n",
    "for url in wf_urls:\n",
    "    var_df = download_data(url)\n",
    "    if len(var_df) == 0:\n",
    "        print(f\"Empty dataframe for URL: {url}\")\n",
    "        continue\n",
    "    url_name = url.split('/')[-2]\n",
    "    variable_name = var_df.name.iloc[0].lower()\n",
    "    sql_variables.append(variable_name)\n",
    "    api_variables.append(url_name)\n",
    "    row_variable_name = variable_name\n",
    "    if (variable_name not in var_df.columns) and (variable_name + \"_from\" not in var_df.columns):\n",
    "        row_variable_name = \"coded_value\"\n",
    "    range_var =  row_variable_name + \"_from\" in var_df.columns\n",
    "    if range_var:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df[row_variable_name + '_from'],\n",
    "            \"value_to\": var_df[row_variable_name + '_to'],\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    else:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df.apply(lambda x: get_code(x[row_variable_name], x['tag']), axis=1),\n",
    "            \"value_to\": np.nan,\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    wf_db = pd.concat([wf_db, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "08de86c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ec_urls = []\n",
    "for col in urls.columns:\n",
    "    if col.split(\"/\")[-2] == \"ec\":\n",
    "        ec_urls.append(urls[col].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea5cab4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 162 rows\n",
      "Downloaded 156 rows\n",
      "Downloaded 140 rows\n",
      "Downloaded 117 rows\n",
      "Downloaded 114 rows\n",
      "Downloaded 99 rows\n",
      "Downloaded 110 rows\n",
      "Downloaded 123 rows\n",
      "Downloaded 83 rows\n",
      "Downloaded 103 rows\n",
      "Downloaded 2 rows\n"
     ]
    }
   ],
   "source": [
    "ec_urls = []\n",
    "for col in urls.columns:\n",
    "    if col.split(\"/\")[-2] == \"ec\":\n",
    "        ec_urls.append(urls[col].values[0])\n",
    "        \n",
    "ec_db =pd.DataFrame()\n",
    "for url in ec_urls:\n",
    "    var_df = download_data(url, size = np.nan)\n",
    "    if len(var_df) == 0:\n",
    "        print(f\"Empty dataframe for URL: {url}\")\n",
    "        continue\n",
    "    url_name = url.split('/')[-2]\n",
    "    variable_name = var_df.name.iloc[0].lower()\n",
    "    sql_variables.append(variable_name)\n",
    "    api_variables.append(url_name)\n",
    "    row_variable_name = variable_name\n",
    "    if (variable_name not in var_df.columns) and (variable_name + \"_from\" not in var_df.columns):\n",
    "        row_variable_name = \"coded_value\"\n",
    "    range_var =  row_variable_name + \"_from\" in var_df.columns\n",
    "    if range_var:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df[row_variable_name + '_from'],\n",
    "            \"value_to\": var_df[row_variable_name + '_to'],\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    else:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df.apply(lambda x: get_code(x[row_variable_name], x['tag']), axis=1),\n",
    "            \"value_to\": np.nan,\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    ec_db = pd.concat([ec_db, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5b777ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "rt_urls = []\n",
    "for col in urls.columns:\n",
    "    if col.split(\"/\")[-2] == \"rt\":\n",
    "\n",
    "        rt_urls.append(urls[col].values[0])\n",
    "religion_urls = rt_urls[:3]\n",
    "msp_urls = rt_urls[4:6] + rt_urls[-12:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4613132b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 272 rows\n",
      "Downloaded 135 rows\n",
      "Downloaded 461 rows\n",
      "Downloaded 457 rows\n",
      "Downloaded 457 rows\n",
      "Downloaded 464 rows\n",
      "Downloaded 463 rows\n",
      "Downloaded 452 rows\n",
      "Downloaded 410 rows\n",
      "Downloaded 412 rows\n",
      "Downloaded 446 rows\n",
      "Downloaded 443 rows\n",
      "Downloaded 463 rows\n",
      "Downloaded 357 rows\n"
     ]
    }
   ],
   "source": [
    "msp_db =pd.DataFrame()\n",
    "for url in msp_urls:\n",
    "    var_df = download_data(url)\n",
    "    if len(var_df) == 0:\n",
    "        print(f\"Empty dataframe for URL: {url}\")\n",
    "        continue\n",
    "    url_name = url.split('/')[-2]\n",
    "    variable_name = var_df.name.iloc[0].lower()\n",
    "    sql_variables.append(variable_name)\n",
    "    api_variables.append(url_name)\n",
    "    row_variable_name = variable_name\n",
    "    if (variable_name.lower() + \"_name\" not in var_df.columns) and (variable_name not in var_df.columns):\n",
    "        if variable_name in column_names:\n",
    "            row_variable_name = column_names[variable_name].lower()\n",
    "        else:\n",
    "            row_variable_name = \"coded_value\"\n",
    "    range_var =  row_variable_name + \"_from\" in var_df.columns\n",
    "    if url not in religion_urls:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df[row_variable_name],\n",
    "            \"value_to\": np.nan,\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    else:\n",
    "        new_df = pd.DataFrame({\n",
    "            \"seshat_region\": var_df.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None),\n",
    "            \"polity_number\": var_df['polity_id'],\n",
    "            \"polity_id\": var_df['polity_name'],\n",
    "            \"section\": url.split('/')[-3],\n",
    "            \"variable_name\": variable_name,\n",
    "            \"value_from\": var_df.apply(lambda x: get_code(x[row_variable_name.lower()+\"_religion_name\"], x['tag']), axis=1),\n",
    "            \"value_to\": np.nan,\n",
    "            \"year_from\": var_df['year_from'],\n",
    "            \"year_to\": var_df['year_to'],\n",
    "            \"is_disputed\": var_df['is_disputed'],\n",
    "            \"is_uncertain\": var_df['is_uncertain']\n",
    "        })\n",
    "    msp_db = pd.concat([msp_db, new_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d24a5d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_df = download_data_json('../datasets/power_transitions.json')\n",
    "# pt_df[\"seshat_region\"] = pt_df.polity_new_ID.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None)\n",
    "# pt_df[\"polity_number\"] = pt_df.polity_new_ID.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"polity_number\"].values[0] if x in polity_df[\"polity_id\"].values else None)\n",
    "# pt_df[\"section\"] = \"crisisdb-pt\"\n",
    "# pt_df[\"variable_name\"] = \"power-transitions\"\n",
    "# pt_df = pt_df[['seshat_region','polity_number','polity_new_ID', 'section', 'variable_name',\n",
    "#        'coded_values_predecessor', 'coded_values_successor',\n",
    "#        'coded_values_contested', 'coded_values_overturn',\n",
    "#        'coded_values_predecessor_assassination', 'coded_values_intra_elite',\n",
    "#        'coded_values_military_revolt', 'coded_values_popular_uprising',\n",
    "#        'coded_values_separatist_rebellion', 'coded_values_external_invasion',\n",
    "#        'coded_values_external_interference', 'year_from', 'year_to', \"is_disputed\", \"is_uncertain\" ]]\n",
    "# pt_df.rename(columns={'polity_new_ID': 'polity_id',\n",
    "#                      'coded_values_predecessor': 'predecessor',\n",
    "#                      'coded_values_successor': 'successor',\n",
    "#                      'coded_values_contested': 'contested',\n",
    "#                      'coded_values_overturn': 'overturn',\n",
    "#                      'coded_values_predecessor_assassination': 'predecessor_assassination',\n",
    "#                      'coded_values_intra_elite': 'intra_elite',\n",
    "#                      'coded_values_military_revolt': 'military_revolt',\n",
    "#                      'coded_values_popular_uprising': 'popular_uprising',\n",
    "#                      'coded_values_separatist_rebellion': 'separatist_rebellion',\n",
    "#                      'coded_values_external_invasion': 'external_invasion',\n",
    "#                      'coded_values_external_interference': 'external_interference'\n",
    "#                     }, inplace=True)\n",
    "# sql_variables.append(\"power-transitions\")\n",
    "# api_variables.append(\"power-transitions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74dff20d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://seshat-db.com/api/crisisdb/us-violence-subtypes/',\n",
       " 'https://seshat-db.com/api/crisisdb/us-violence-data-sources/',\n",
       " 'https://seshat-db.com/api/crisisdb/us-violences/',\n",
       " 'https://seshat-db.com/api/crisisdb/crisis-consequences/']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crisis_urls = []\n",
    "for col in urls.columns:\n",
    "\n",
    "    if col.split(\"/\")[-2] == \"crisisdb\":\n",
    "        crisis_urls.append(urls[col].values[0])\n",
    "crisis_urls = crisis_urls[1:]\n",
    "crisis_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b59442d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 169 rows\n"
     ]
    }
   ],
   "source": [
    "crisis_consequences_db = download_data(crisis_urls[3])\n",
    "crisis_consequences_db[\"seshat_region\"] = crisis_consequences_db.polity_name.apply(lambda x: polity_df.loc[polity_df['polity_id'] == x, \"seshat_region\"].values[0] if x in polity_df[\"polity_id\"].values else None)\n",
    "crisis_consequences_db[\"polity_number\"] = crisis_consequences_db.polity_id\n",
    "crisis_consequences_db.drop(columns=['polity_id'], inplace=True)\n",
    "crisis_consequences_db[\"section\"] = \"crisisdb-cc\"\n",
    "crisis_consequences_db[\"variable_name\"] = \"crisis-consequences\"\n",
    "crisis_consequences_db = crisis_consequences_db[['seshat_region','polity_number','polity_name', 'section', 'variable_name', \"other_polity_name\", \"other_polity_id\",'decline',\n",
    "       'collapse', 'epidemic', 'downward_mobility', 'extermination',\n",
    "       'uprising', 'revolution', 'successful_revolution', 'civil_war',\n",
    "       'century_plus', 'fragmentation', 'capital', 'conquest', 'assassination',\n",
    "       'depose', 'constitution', 'labor', 'unfree_labor', 'suffrage',\n",
    "       'public_goods', 'religion',\"year_from\", \"year_to\", 'is_disputed', 'is_uncertain']] \n",
    "crisis_consequences_db = crisis_consequences_db.rename(\n",
    "    columns={'polity_name': 'polity_id', \n",
    "             'other_polity_name': 'other_polity_id',\n",
    "             'other_polity_id': 'other_polity_number'})\n",
    "sql_variables.append(\"crisis-consequences\")\n",
    "api_variables.append(\"crisis-consequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a3a835e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RA', 'Dogs']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "equinox_db = pd.read_excel('..\\datasets\\Equinox2020.05.2023.xlsx')\n",
    "new_columns = equinox_db.Variable.unique()\n",
    "missing_columns = []\n",
    "sql_col_mappings = {}\n",
    "for col in new_columns:\n",
    "    best_match = ''\n",
    "    best_match_length = 0\n",
    "    for sql_col in sql_variables:\n",
    "    \n",
    "        match_length = len(longest_substring_finder(col, sql_col.replace(\" \", \"-\").lower()))\n",
    "        if match_length > best_match_length:\n",
    "            best_match = sql_col\n",
    "            best_match_length = match_length\n",
    "    \n",
    "    if (best_match_length/len(col) > 0.5) or (len(best_match) > 7):\n",
    "        sql_col_mappings[col] = best_match\n",
    "    else:\n",
    "        missing_columns.append(col)\n",
    "\n",
    "missing_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f714cbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mappings_to_old_names = {\n",
    "    ## General variables\n",
    "    \"research_assistant\": \"RA\",\n",
    "    \"expert\": \"Expert\",\n",
    "    \"duration\": \"Duration\", #\n",
    "    \"peak_years\": \"Peak Date\", #\n",
    "\n",
    "    \"administrative_level\": \"AdmLev\", # \"Administrative levels\"\n",
    "    \"article\": \"Article\", # \"Articles\"\n",
    "    \"atlatl\": \"Atlatl\", # \"Atlatl\"\n",
    "    \"battle_axe\": \"BattleAxe\", # \"Battle axes\"\n",
    "    \"breastplate\": \"Breastplate\", # \"Breastplates\"\n",
    "    \"bridge\": \"Bridge\", # \"Bridges\"\n",
    "    \"bronze\": \"Bronze\", # \"Bronze\"\n",
    "    \"calendar\": \"Calendar\", # \"Calendar\"\n",
    "    \"camel\": \"Camel\", # \"Camels\"\n",
    "    \"canal\": \"Canal\", # \"Canals\"\n",
    "    \"chainmail\": \"Chainmail\", # \"Chainmail\"\n",
    "    \"complex_fortification\": \"ComplxFort\", # \"Complex fortifications\"\n",
    "    \"composite_bow\": \"CompBow\", # \"Composite bow\"\n",
    "    \"copper\": \"Copper\", # \"Copper\"\n",
    "    \"courier\": \"Courier\", # \"Couriers\"\n",
    "    \"court\": \"Court\", # \"Courts\"\n",
    "    \"crossbow\": \"Crossbow\", # \"Crossbow\"\n",
    "    \"dagger\": \"Dagger\", # \"Daggers\"\n",
    "    \"ditch\": \"Ditch\", # \"Ditch\"\n",
    "    \"dog\": \"Dog\", # \"Dogs\"\n",
    "    \"donkey\": \"Donkey\", # \"Donkeys\"\n",
    "    \"drinking_water_supply_system\": \"WaterSuppl\", # \"drinking water supply systems\"\n",
    "    \"earth_rampart\": \"Rampart\", # \"Earth ramparts\"\n",
    "    \"elephant\": \"Elephant\", # \"Elephants\"\n",
    "    \"examination_system\": \"ExamSyst\", # \"Examination system\"\n",
    "    \"fiction\": \"Fiction\", # \"Fiction\"\n",
    "    \"food_storage_site\": \"FoodStor\", # \"food storage sites\"\n",
    "    \"foreign_coin\": \"ForCoin\", # \"Foreign coins\"\n",
    "    \"formal_legal_code\": \"LegCode\", # \"Formal legal code\"\n",
    "    \"fortified_camp\": \"FortCamp\", # \"Fortified camps\"\n",
    "    \"full_time_bureaucrat\": \"FullTBur\", # \"Full-time bureaucrats\"\n",
    "    \"general_postal_service\": \"General postal service\",\n",
    "    \"gunpowder_siege_artillery\": \"Gunpowder siege artillery\",\n",
    "    \"handheld_firearm\": \"HandGun\", # \"Handheld firearms\"\n",
    "    \"helmet\": \"Helmet\", # \"Helmets\"\n",
    "    \"history\": \"History\", # \"History\"\n",
    "    \"horse\": \"Horse\", # \"Horses\"\n",
    "    \"indigenous_coin\": \"IndigCoin\", # \"Indigenous coins\"\n",
    "    \"iron\": \"Iron\", # \"Iron\"\n",
    "    \"irrigation_system\": \"Irrigation\", # \"irrigation systems\"\n",
    "    \"javelin\": \"Javelin\", # \"Javelins\"\n",
    "    \"judge\": \"Judge\", # \"Judges\"\n",
    "    \"laminar_armor\": \"LaminArmor\", # \"Laminar armor\"\n",
    "    \"leather_cloth\": \"LeathArmor\", # \"Leather cloth\"\n",
    "    \"limb_protection\": \"LimbProt\", # \"Limb protection\"\n",
    "    \"lists_tables_and_classification\": \"Lists\", # \"Lists tables and classifications\"\n",
    "    \"long_wall\": \"LongWall\", # \"Long walls\"\n",
    "    \"market\": \"Market\", # \"markets\"\n",
    "    \"merchant_ships_pressed_into_service\": \"Merchant\", # \"Merchant ships pressed into service\"\n",
    "    \"merit_promotion\": \"MeritProm\", # \"Merit promotion\"\n",
    "    \"military_level\": \"MilLev\", # \"Military levels\"\n",
    "    \"mines_or_quarry\": \"Mine\", # \"Mines or quarries\"\n",
    "    \"mnemonic_device\": \"Mnemonic\", # \"Mnemonic devices\"\n",
    "    \"moat\": \"Moat\", # \"Moat\"\n",
    "    \"modern_fortification\": \"ModernFort\", # \"Modern fortifications\"\n",
    "    \"non_phonetic_writing\": \"NonPhWrit\", # \"Non-phonetic.alphabetic.writing\"\n",
    "    \"nonwritten_record\": \"NonWRecord\", # \"Nonwritten records\"\n",
    "    \"paper_currency\": \"PaperCurr\", # \"Paper currency\"\n",
    "    \"philosophy\": \"Philosophy\", # \"Philosophy\"\n",
    "    \"phonetic_alphabetic_writing\": \"PhAlph\", # \"Phonetic alphabetic writing\"\n",
    "    \"plate_armor\": \"PlateArmor\", # \"Plate armor\"\n",
    "    \"polearm\": \"Polearm\", # \"Polearms\"\n",
    "    \"polity_population\": \"PolPop\", # \"Polity Population\"\n",
    "    \"polity_territory\": \"PolTerr\", # \"Polity territory\"\n",
    "    \"population_of_the_largest_settlement\": \"Population of the largest settlement\", # ???\n",
    "    \"port\": \"Port\", # \"Ports\"\n",
    "    \"postal_station\": \"PostStation\", # \"Postal stations\"\n",
    "    \"practical_literature\": \"PractLit\", # \"Practical literature\"\n",
    "    \"precious_metal\": \"PrecMetal\", # \"Precious metals\"\n",
    "    \"professional_lawyer\": \"Professional Lawyers\",\n",
    "    \"professional_military_officer\": \"ProfOfficer\", # \"Professional military officers\"\n",
    "    \"professional_priesthood\": \"ProfPriest\", # \"Professional priesthood\"\n",
    "    \"professional_soldier\": \"ProfSoldier\", # \"Professional soldiers\"\n",
    "    \"religious_level\": \"ReligLev\", # \"Religious levels\"\n",
    "    \"religious_literature\": \"ReligLit\", # \"Religious literature\"\n",
    "    \"road\": \"Road\", # \"Roads\"\n",
    "    \"sacred_text\": \"SacrTxt\", # \"Sacred Texts\"\n",
    "    \"scaled_armor\": \"ScaleArmor\", # \"Scaled armor\"\n",
    "    \"scientific_literature\": \"SciLit\", # \"Scientific literature\"\n",
    "    \"script\": \"Script\", # \"Script\"\n",
    "    \"self_bow\": \"SelfBow\", # \"Self bow\"\n",
    "    \"settlement_hierarchy\": \"SettlHier\", # \"Settlement hierarchy\"\n",
    "    \"settlements_in_a_defensive_position\": \"DefPosition\", # \"Settlements in a defensive position\"\n",
    "    \"shield\": \"Shield\", # \"Shields\"\n",
    "    \"sling_siege_engine\": \"SlingSiege\", # \"Sling siege engines\"\n",
    "    \"sling\": \"Sling\", # \"Slings\"\n",
    "    \"small_vessels_canoes_etc\": \"SmallVessel\", # \"Small vessels (canoes etc)\"\n",
    "    \"spear\": \"Spear\", # \"Spears\"\n",
    "    \"specialized_government_building\": \"Specialized government buildings\",\n",
    "    \"specialized_military_vessel\": \"SpecVessel\", # \"Specialized military vessels\"\n",
    "    \"steel\": \"Steel\", # \"Steel\"\n",
    "    \"stone_walls_mortared\": \"StoneWall\", # \"Stone walls (mortared)\"\n",
    "    \"stone_walls_non_mortared\": \"NonMStone\", # \"Stone walls (non-mortared)\"\n",
    "    \"sword\": \"Sword\", # \"Swords\"\n",
    "    \"tension_siege_engine\": \"TensSiege\", # \"Tension siege engines\"\n",
    "    \"token\": \"Token\", # \"Tokens\"\n",
    "    \"war_club\": \"WarClub\", # \"War clubs\"\n",
    "    \"wood_bark_etc\": \"WoodArmor\", # \"Wood bark etc\"\n",
    "    \"wooden_palisade\": \"Palisade\", # \"Wooden palisades\"\n",
    "    \"written_record\": \"WRecord\", # \"Written records\"\n",
    "    ## Add these MSP variables, which will be renamed (left-hand side) by Majid when they are added to SQL\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3ef70ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_to_api_mappings = {}\n",
    "for sql,api in zip(sql_variables, api_variables):\n",
    "    sql_to_api_mappings[sql] = api\n",
    "\n",
    "unique_vars = sc_db.variable_name.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "692d6806",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_pols = []\n",
    "for pol in polity_df.polity_id.unique():\n",
    "    if pol not in wf_db.polity_id.unique():\n",
    "        if pol not in sc_db.polity_id.unique():\n",
    "            if pol not in general_db.polity_id.unique():\n",
    "                if pol not in ec_db.polity_id.unique():\n",
    "                    if pol not in msp_db.polity_id.unique():\n",
    "                        if pol not in crisis_consequences_db.polity_id.unique():\n",
    "                            if pol not in crisis_consequences_db.other_polity_id.unique():\n",
    "                                missing_pols.append(pol)\n",
    "\n",
    "polity_df = polity_df[~polity_df.polity_id.isin(missing_pols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ad66354",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_to_equinox_mappings = {}\n",
    "for col in sql_col_mappings:\n",
    "    if sql_col_mappings[col] in sql_variables:\n",
    "        sql_to_equinox_mappings[sql_col_mappings[col]] = col\n",
    "    else:\n",
    "        print(f\"Column {col} not found in SQL variables.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "634aaea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_db = pd.DataFrame(columns=[\"variable_name\", \"section\", \"sheet_name\", 'api_variable_name', \"equinox_variable_name\",\"old_variable_name\"])\n",
    "sheet_names = [\"General\", \"Social complexity\", \"Warfare\", \"Luxury goods\", \"Religion\", \"CrisisDB - Crisis consequences\", \"CrisisDB - Power transitions\"]\n",
    "\n",
    "\n",
    "for n,df in enumerate([general_db, sc_db, wf_db, ec_db, msp_db, crisis_consequences_db]):\n",
    "    unique_vars = df.variable_name.unique()\n",
    "    sql_vars = [x if x in sql_to_api_mappings else 'not found' for x in unique_vars]\n",
    "    api_vars = [sql_to_api_mappings[x] if x in sql_to_api_mappings else 'not found' for x in unique_vars]\n",
    "    equinox_variables = [sql_to_equinox_mappings[x] if x in sql_to_equinox_mappings else 'not found' for x in sql_vars]\n",
    "    old_vars = [col_mappings_to_old_names[x] if x in col_mappings_to_old_names else 'not found' for x in sql_vars]\n",
    "    new_df = pd.DataFrame({\n",
    "        \"variable_name\": sql_vars,\n",
    "        \"section\": df.section.iloc[0],\n",
    "        \"sheet_name\": sheet_names[n],\n",
    "        \"api_variable_name\": unique_vars,\n",
    "        \"equinox_variable_name\": equinox_variables,\n",
    "        \"old_variable_name\": old_vars\n",
    "    })\n",
    "    variables_db = pd.concat([variables_db, new_df], ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "26a5b7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables_db.drop([\"section\", \"sheet_name\"], axis=1, inplace=True)\n",
    "variables_db = variables_db.rename(columns={\"variable_name\": \"api_variable_name\"})\n",
    "variables_db.to_csv(\"../datasets/variables.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3d179fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "with pd.ExcelWriter('..\\datasets\\\\Polaris2025_v2.xlsx', engine=\"openpyxl\") as writer:\n",
    "\n",
    "    variables_db.to_excel(writer, sheet_name=\"Variables\", index=False)\n",
    "    polity_df.to_excel(writer, sheet_name=\"Polities\", index=False)\n",
    "    threads.to_excel(writer, sheet_name=\"Polity threads\", index=False)\n",
    "    general_db.to_excel(writer, sheet_name=sheet_names[0], index=False)\n",
    "    sc_db.to_excel(writer, sheet_name=sheet_names[1], index=False)\n",
    "    wf_db.to_excel(writer, sheet_name=sheet_names[2], index=False)\n",
    "    ec_db.to_excel(writer, sheet_name=sheet_names[3], index=False)\n",
    "    msp_db.to_excel(writer, sheet_name=sheet_names[4], index=False)\n",
    "    crisis_consequences_db.to_excel(writer, sheet_name=sheet_names[5], index=False)\n",
    "    # pt_df.to_excel(writer, sheet_name=sheet_names[6], index=False)    \n",
    "\n",
    "# adjust the column widths\n",
    "    for sheet_name in writer.sheets:\n",
    "        worksheet = writer.sheets[sheet_name]\n",
    "        for col_idx, col_cells in enumerate(worksheet.iter_cols(), start=1):\n",
    "            max_length = 0\n",
    "            for cell in col_cells:\n",
    "                try:\n",
    "                    if cell.value is not None and len(str(cell.value)) > max_length:\n",
    "                        max_length = len(str(cell.value))\n",
    "                except:\n",
    "                    pass\n",
    "            adjusted_width = np.clip(10,max_length*0.8 + 0.1,40)\n",
    "            worksheet.column_dimensions[get_column_letter(col_idx)].width = adjusted_width"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
