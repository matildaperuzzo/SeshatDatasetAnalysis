{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'src')))\n",
    "\n",
    "# Now you can import the TimeSeriesDataset class\n",
    "from TimeSeriesDataset import TimeSeriesDataset as TSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 839 rows\n",
      "Downloaded 464 rows\n",
      "Downloaded sc/polity-territories dataset with 464 rows in 39.631311893463135 seconds\n",
      "Downloaded 440 rows\n",
      "Downloaded sc/polity-populations dataset with 440 rows in 46.26707911491394 seconds\n",
      "Downloaded 492 rows\n",
      "Downloaded sc/population-of-the-largest-settlements dataset with 492 rows in 40.329955101013184 seconds\n",
      "Downloaded 474 rows\n",
      "Downloaded sc/settlement-hierarchies dataset with 474 rows in 29.565491914749146 seconds\n",
      "Downloaded 0 rows\n",
      "Downloaded sc/administrative-levels dataset with 0 rows in 1.2806642055511475 seconds\n",
      "Empty dataset for sc/administrative-levels\n",
      "Downloaded 351 rows\n",
      "Downloaded sc/religious-levels dataset with 351 rows in 28.29832434654236 seconds\n",
      "Downloaded 364 rows\n",
      "Downloaded sc/military-levels dataset with 364 rows in 17.50086522102356 seconds\n",
      "Downloaded 381 rows\n",
      "Downloaded sc/professional-military-officers dataset with 381 rows in 17.62206792831421 seconds\n",
      "Downloaded 409 rows\n",
      "Downloaded sc/professional-soldiers dataset with 409 rows in 19.70322585105896 seconds\n",
      "Downloaded 388 rows\n",
      "Downloaded sc/professional-priesthoods dataset with 388 rows in 20.055129051208496 seconds\n",
      "Downloaded 70 rows\n",
      "Downloaded sc/full-time-bureaucrats dataset with 70 rows in 29.374953985214233 seconds\n",
      "Downloaded 330 rows\n",
      "Downloaded sc/examination-systems dataset with 330 rows in 18.502249002456665 seconds\n",
      "Downloaded 319 rows\n",
      "Downloaded sc/merit-promotions dataset with 319 rows in 17.35026502609253 seconds\n",
      "Downloaded 407 rows\n",
      "Downloaded sc/specialized-government-buildings dataset with 407 rows in 39.84141492843628 seconds\n",
      "Downloaded 270 rows\n",
      "Downloaded sc/formal-legal-codes dataset with 270 rows in 28.224215030670166 seconds\n",
      "Downloaded 20 rows\n",
      "Downloaded sc/judges dataset with 20 rows in 6.779618978500366 seconds\n",
      "Downloaded 359 rows\n",
      "Downloaded sc/courts dataset with 359 rows in 22.16295027732849 seconds\n",
      "Downloaded 327 rows\n",
      "Downloaded sc/professional-lawyers dataset with 327 rows in 30.79748272895813 seconds\n",
      "Downloaded 402 rows\n",
      "Downloaded sc/irrigation-systems dataset with 402 rows in 26.46399712562561 seconds\n",
      "Downloaded 326 rows\n",
      "Downloaded sc/drinking-water-supplies dataset with 326 rows in 20.688286066055298 seconds\n",
      "Downloaded 421 rows\n",
      "Downloaded sc/markets dataset with 421 rows in 32.9595582485199 seconds\n",
      "Downloaded 375 rows\n",
      "Downloaded sc/food-storage-sites dataset with 375 rows in 27.126943111419678 seconds\n",
      "Downloaded 150 rows\n",
      "Downloaded sc/roads dataset with 150 rows in 27.00672698020935 seconds\n",
      "Downloaded 329 rows\n",
      "Downloaded sc/bridges dataset with 329 rows in 77.25490188598633 seconds\n",
      "Downloaded 0 rows\n",
      "Downloaded sc/canals dataset with 0 rows in 6.270398855209351 seconds\n",
      "Empty dataset for sc/canals\n",
      "Downloaded 369 rows\n",
      "Downloaded sc/ports dataset with 369 rows in 106.76964092254639 seconds\n",
      "Downloaded 325 rows\n",
      "Downloaded sc/mines-or-quarries dataset with 325 rows in 19.252827167510986 seconds\n",
      "Downloaded 40 rows\n",
      "Downloaded sc/mnemonic-devices dataset with 40 rows in 4.984123945236206 seconds\n",
      "Downloaded 302 rows\n",
      "Downloaded sc/nonwritten-records dataset with 302 rows in 19.396332025527954 seconds\n",
      "Downloaded 476 rows\n",
      "Downloaded sc/written-records dataset with 476 rows in 27.2678279876709 seconds\n",
      "Downloaded 474 rows\n",
      "Downloaded sc/scripts dataset with 474 rows in 23.782771825790405 seconds\n",
      "Downloaded 292 rows\n",
      "Downloaded sc/non-phonetic-writings dataset with 292 rows in 16.10548186302185 seconds\n",
      "Downloaded 428 rows\n",
      "Downloaded sc/phonetic-alphabetic-writings dataset with 428 rows in 25.133096933364868 seconds\n",
      "Downloaded 409 rows\n",
      "Downloaded sc/lists-tables-and-classifications dataset with 409 rows in 21.298663854599 seconds\n",
      "Downloaded 446 rows\n",
      "Downloaded sc/calendars dataset with 446 rows in 36.66166114807129 seconds\n",
      "Downloaded 290 rows\n",
      "Downloaded sc/sacred-texts dataset with 290 rows in 43.33597993850708 seconds\n",
      "Downloaded 390 rows\n",
      "Downloaded sc/religious-literatures dataset with 390 rows in 89.65478205680847 seconds\n",
      "Downloaded 408 rows\n",
      "Downloaded sc/practical-literatures dataset with 408 rows in 169.57729697227478 seconds\n",
      "Downloaded 396 rows\n",
      "Downloaded sc/histories dataset with 396 rows in 18.720661163330078 seconds\n",
      "Downloaded 374 rows\n",
      "Downloaded sc/philosophies dataset with 374 rows in 33.761380195617676 seconds\n",
      "Downloaded 388 rows\n",
      "Downloaded sc/scientific-literatures dataset with 388 rows in 19.343698263168335 seconds\n",
      "Downloaded 399 rows\n",
      "Downloaded sc/fictions dataset with 399 rows in 19.39064908027649 seconds\n",
      "Downloaded 391 rows\n",
      "Downloaded sc/articles dataset with 391 rows in 22.97144079208374 seconds\n",
      "Downloaded 293 rows\n",
      "Downloaded sc/tokens dataset with 293 rows in 17.829343795776367 seconds\n",
      "Downloaded 320 rows\n",
      "Downloaded sc/precious-metals dataset with 320 rows in 18.755160093307495 seconds\n",
      "Downloaded 367 rows\n",
      "Downloaded sc/foreign-coins dataset with 367 rows in 33.04558801651001 seconds\n",
      "Downloaded 424 rows\n",
      "Downloaded sc/indigenous-coins dataset with 424 rows in 30.14271306991577 seconds\n",
      "Downloaded 378 rows\n",
      "Downloaded sc/paper-currencies dataset with 378 rows in 33.1910982131958 seconds\n",
      "Downloaded 331 rows\n",
      "Downloaded sc/couriers dataset with 331 rows in 88.22228193283081 seconds\n",
      "Downloaded 299 rows\n",
      "Downloaded sc/postal-stations dataset with 299 rows in 28.669250011444092 seconds\n",
      "Downloaded 311 rows\n",
      "Downloaded sc/general-postal-services dataset with 311 rows in 44.547096252441406 seconds\n"
     ]
    }
   ],
   "source": [
    "# initialize dataset by downloading dataset or downloading the data from polity_url\n",
    "dataset = TSD(categories=['sc'])\n",
    "# download all datasets\n",
    "dataset.download_all_categories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polity gb_british_emp_2 (1900.0 - 1900.0) is inactive but has data for sc/polity-territories from 1800.0 to 1800.0\n",
      "Polity gb_british_emp_2 (1900.0 - 1900.0) is inactive but has data for sc/polity-territories_from from 1800.0 to 1800.0\n",
      "Polity gb_british_emp_2 (1900.0 - 1900.0) is inactive but has data for sc/polity-territories_to from 1800.0 to 1800.0\n",
      "Polity jp_ashikaga (1400.0 - 1400.0) is inactive but has data for sc/polity-populations from 1500.0 to 1500.0\n",
      "Polity us_early_illinois_confederation (1700.0 - 1700.0) is inactive but has data for sc/polity-populations from 1800.0 to 1800.0\n",
      "Polity jp_ashikaga (1400.0 - 1400.0) is inactive but has data for sc/polity-populations_from from 1500.0 to 1500.0\n",
      "Polity us_early_illinois_confederation (1700.0 - 1700.0) is inactive but has data for sc/polity-populations_from from 1800.0 to 1800.0\n",
      "Polity jp_ashikaga (1400.0 - 1400.0) is inactive but has data for sc/polity-populations_to from 1500.0 to 1500.0\n",
      "Polity us_early_illinois_confederation (1700.0 - 1700.0) is inactive but has data for sc/polity-populations_to from 1800.0 to 1800.0\n",
      "Polity jp_ashikaga (1400.0 - 1400.0) is inactive but has data for sc/population-of-the-largest-settlements from 1500.0 to 1500.0\n",
      "Polity gb_british_emp_2 (1900.0 - 1900.0) is inactive but has data for sc/population-of-the-largest-settlements from 1800.0 to 1800.0\n",
      "Polity it_roman_rep_2 (-200.0 - -200.0) is inactive but has data for sc/population-of-the-largest-settlements from -100.0 to -100.0\n",
      "Polity jp_ashikaga (1400.0 - 1400.0) is inactive but has data for sc/population-of-the-largest-settlements_from from 1500.0 to 1500.0\n",
      "Polity gb_british_emp_2 (1900.0 - 1900.0) is inactive but has data for sc/population-of-the-largest-settlements_from from 1800.0 to 1800.0\n",
      "Polity it_roman_rep_2 (-200.0 - -200.0) is inactive but has data for sc/population-of-the-largest-settlements_from from -100.0 to -100.0\n",
      "Polity jp_ashikaga (1400.0 - 1400.0) is inactive but has data for sc/population-of-the-largest-settlements_to from 1500.0 to 1500.0\n",
      "Polity gb_british_emp_2 (1900.0 - 1900.0) is inactive but has data for sc/population-of-the-largest-settlements_to from 1800.0 to 1800.0\n",
      "Polity it_roman_rep_2 (-200.0 - -200.0) is inactive but has data for sc/population-of-the-largest-settlements_to from -100.0 to -100.0\n",
      "-10000.0 1900.0\n",
      "-10000.0 1900.0\n",
      "-10000.0 1900.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mperuzzo/Documents/repos/SeshatDatasetAnalysis/src/TimeSeriesDataset.py:275: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.raw.drop(columns = ['PolityActive'], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# clean up dataser and create a debug dataset dataset.debug\n",
    "dataset.debug_clean_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already cleaned\n"
     ]
    }
   ],
   "source": [
    "# if you try to debug the dataset twice it will inform you\n",
    "dataset.debug_clean_dataset()\n",
    "# save dataset\n",
    "dataset.save_dataset(path = \"../datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['sc/administrative-levels', 'sc/canals'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m dataset\u001b[38;5;241m.\u001b[39mload_dataset(path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# remove all rows that have less than 30% of the columns filled in\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mremove_incomplete_rows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnan_threshold\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# build the social complexity variables\u001b[39;00m\n\u001b[1;32m      8\u001b[0m dataset\u001b[38;5;241m.\u001b[39mbuild_social_complexity()\n",
      "File \u001b[0;32m~/Documents/repos/SeshatDatasetAnalysis/src/TimeSeriesDataset.py:286\u001b[0m, in \u001b[0;36mTimeSeriesDataset.remove_incomplete_rows\u001b[0;34m(self, nan_threshold)\u001b[0m\n\u001b[1;32m    283\u001b[0m     cols \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msc/\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m+\u001b[39m key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(social_complexity_mapping[key]\u001b[38;5;241m.\u001b[39mkeys())]\n\u001b[1;32m    285\u001b[0m \u001b[38;5;66;03m# remove rows with less than 30% of the columns filled in\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraw\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcols\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mnotna()\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(cols)\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m0.3\u001b[39m]\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Documents/repos/SeshatDatasetAnalysis/.venv/lib/python3.11/site-packages/pandas/core/frame.py:3813\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3811\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3812\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3813\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3815\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m~/Documents/repos/SeshatDatasetAnalysis/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6070\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6067\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6068\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6070\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6072\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6074\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/repos/SeshatDatasetAnalysis/.venv/lib/python3.11/site-packages/pandas/core/indexes/base.py:6133\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6130\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6132\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6133\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['sc/administrative-levels', 'sc/canals'] not in index\""
     ]
    }
   ],
   "source": [
    "# import dataset\n",
    "dataset.load_dataset(path = \"../datasets\")\n",
    "\n",
    "\n",
    "# remove all rows that have less than 30% of the columns filled in\n",
    "dataset.remove_incomplete_rows(nan_threshold=0.3)\n",
    "# build the social complexity variables\n",
    "dataset.build_social_complexity()\n",
    "# impute missing data\n",
    "dataset.impute_missing_values()\n",
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
